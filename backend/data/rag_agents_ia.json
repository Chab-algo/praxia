{
  "documents": [
    {"content": "Un agent IA est un système logiciel qui perçoit son environnement, prend des décisions et exécute des actions pour atteindre des objectifs. Il combine typiquement un modèle de langage (LLM), des outils (tools) et une boucle de raisonnement (orchestration).", "metadata": {"source": "agents_ia", "type": "definition", "title": "Définition d'un agent IA"}},
    {"content": "L'orchestration d'un agent désigne la logique qui enchaîne appels LLM, décisions et utilisation d'outils. Elle peut être simple (un seul appel) ou complexe (boucle ReAct, plan-and-execute). La qualité de l'orchestration détermine la fiabilité et le coût de l'agent.", "metadata": {"source": "agents_ia", "type": "definition", "title": "Orchestration des agents"}},
    {"content": "Les outils (tools) permettent à l'agent d'agir sur le monde : recherche web, calculs, appels API, lecture de bases de données. Chaque outil est décrit par un schéma (nom, description, paramètres) pour que le LLM sache quand et comment l'appeler.", "metadata": {"source": "agents_ia", "type": "definition", "title": "Outils (tools) des agents"}},
    {"content": "La boucle de décision d'un agent consiste à : (1) recevoir une requête ou un état, (2) décider de la prochaine action (appel LLM ou outil), (3) exécuter l'action, (4) intégrer le résultat et répéter ou terminer. Des timeouts et un nombre max d'étapes évitent les boucles infinies.", "metadata": {"source": "agents_ia", "type": "definition", "title": "Boucle de décision"}},
    {"content": "Le pattern ReAct (Reasoning + Acting) alterne raisonnement en langage naturel et actions concrètes. Le LLM produit des pensées (reasoning) puis des actions (tool calls) ; l'exécuteur renvoie les résultats au LLM pour la suite. ReAct améliore la traçabilité et la correction d'erreurs.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Pattern ReAct"}},
    {"content": "En RAG (Retrieval Augmented Generation), on récupère des documents pertinents via une recherche vectorielle (ou hybride), on les injecte dans le contexte du LLM, puis on génère la réponse. Le RAG réduit les hallucinations en ancrant la réponse dans des sources vérifiables.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Pattern RAG"}},
    {"content": "Le tool use (utilisation d'outils) est le mécanisme par lequel un LLM demande l'exécution d'une fonction : le modèle renvoie un appel structuré (nom de l'outil, arguments), l'agent exécute et renvoie le résultat au LLM. Les modèles récents (GPT-4, Claude) supportent nativement les tool calls.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Tool use"}},
    {"content": "Le pattern plan-and-execute consiste à d'abord produire un plan (liste d'étapes) puis à exécuter chaque étape. Il est utile pour les tâches longues ou multi-étapes. Le plan peut être révisé si une étape échoue ou si les résultats suggèrent un ajustement.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Plan-and-execute"}},
    {"content": "Un agent conversationnel (chatbot) maintient un historique des messages (user/assistant) et optionnellement des résultats d'outils. Le contexte est tronqué ou résumé quand il dépasse la fenêtre du modèle. Les bonnes pratiques incluent un system prompt stable et une gestion explicite des tours de dialogue.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Agent conversationnel"}},
    {"content": "La recherche vectorielle repose sur l'embedding de textes en vecteurs de dimension fixe (ex. 1536 pour text-embedding-3-small). La similarité entre la requête et les documents est mesurée par similarité cosine ou produit scalaire. On retourne les k documents les plus similaires.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Recherche vectorielle"}},
    {"content": "Bonnes pratiques agents IA : définir clairement le périmètre et les limites de l'agent, fournir un system prompt précis, limiter le nombre d'outils et leur description pour éviter les appels erronés, et prévoir des timeouts et un budget d'étapes maximum.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Périmètre et limites"}},
    {"content": "Gestion des erreurs : lorsqu'un outil échoue (API indisponible, paramètre invalide), l'agent doit recevoir un message d'erreur clair et pouvoir réessayer ou s'arrêter proprement. Éviter d'exposer des stack traces brutes au LLM ; formater des messages utilisables pour la décision.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Gestion des erreurs"}},
    {"content": "Timeouts et retries : chaque appel LLM ou outil doit avoir un timeout (ex. 30 s). Pour les opérations idempotentes, un retry avec backoff exponentiel améliore la robustesse. Limiter le nombre de retries pour ne pas bloquer l'utilisateur.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Timeouts et retries"}},
    {"content": "Coûts et tokens : suivre le nombre de tokens en entrée et en sortie (prompt + completion) et le coût par modèle. Fixer un budget par session ou par utilisateur. Réduire les coûts en utilisant des modèles plus petits pour des sous-tâches, en mettant en cache les réponses ou en tronquant le contexte.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Coûts et tokens"}},
    {"content": "Sécurité des données : ne pas envoyer de données sensibles (PII, secrets) dans les prompts sans politique claire. Utiliser des modèles déployés en privé ou des API avec garanties de non-rétention si nécessaire. Auditer les logs pour détecter les fuites.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Sécurité des données"}},
    {"content": "Réduction des hallucinations : ancrer les réponses dans des sources (RAG), demander au modèle de citer ses sources, utiliser un seuil de confiance ou un second passage de vérification pour les affirmations critiques. Un prompt explicite du type « réponds uniquement à partir du contexte » limite les inventions.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Réduction des hallucinations"}},
    {"content": "LangChain est une bibliothèque pour construire des applications pilotées par LLM. Elle fournit des abstractions : modèles (ChatOpenAI, etc.), prompts (ChatPromptTemplate), chaînes (LCEL), retriever, agents et outils. L'utilisation de LCEL (pipe |) permet de composer des pipelines de manière déclarative.", "metadata": {"source": "agents_ia", "type": "langchain", "title": "Présentation LangChain"}},
    {"content": "Dans LangChain, un Retriever est un composant qui prend une requête et retourne une liste de Documents. Il est souvent utilisé avec un VectorStore (embeddings + similarité). BaseRetriever expose get_relevant_documents (sync) et aget_relevant_documents (async) pour s'intégrer dans une chaîne.", "metadata": {"source": "agents_ia", "type": "langchain", "title": "Retriever LangChain"}},
    {"content": "Une Chain LangChain enchaîne plusieurs étapes (prompt, modèle, output parser). Avec LCEL, on compose avec | : par exemple retriever | format_docs | prompt | llm | parser. Chaque étape consomme la sortie de la précédente. Les chaînes sont invocables avec .invoke() ou .ainvoke().", "metadata": {"source": "agents_ia", "type": "langchain", "title": "Chains et LCEL"}},
    {"content": "ChatPromptTemplate permet de construire des messages (system, human, assistant) avec des variables. Exemple : ChatPromptTemplate.from_messages([('system', 'Tu es...'), ('human', '{question}')]). format_messages ou invoke remplissent les variables. Utiliser des templates pour garder les prompts maintenables.", "metadata": {"source": "agents_ia", "type": "langchain", "title": "ChatPromptTemplate"}},
    {"content": "Les output parsers (StrOutputParser, PydanticOutputParser, etc.) transforment la sortie brute du LLM en structure utilisable. StrOutputParser renvoie une chaîne ; PydanticOutputParser parse du JSON vers un modèle Pydantic. Ils s'intègrent en fin de chaîne : llm | parser.", "metadata": {"source": "agents_ia", "type": "langchain", "title": "Output parsers"}},
    {"content": "OpenAIEmbeddings dans LangChain (langchain-openai) permet d'embedder des textes avec l'API OpenAI (modèle text-embedding-3-small ou ada). Méthodes : embed_documents(liste de textes) pour les chunks, embed_query(texte) pour la requête. Les dimensions (ex. 1536) doivent être cohérentes entre ingestion et requête.", "metadata": {"source": "agents_ia", "type": "langchain", "title": "OpenAIEmbeddings"}},
    {"content": "Bonnes pratiques LangChain : réutiliser une instance de modèle ou d'embeddings plutôt que d'en créer à chaque appel ; utiliser les versions async (ainvoke, aembed_documents) dans une API async ; gérer les erreurs et timeouts au niveau du client HTTP sous-jacent.", "metadata": {"source": "agents_ia", "type": "langchain", "title": "Bonnes pratiques LangChain"}},
    {"content": "Évaluation des agents : mesurer la pertinence des réponses (relevance), la fidélité aux sources (groundedness), et le taux de succès des tâches (task success). Les métriques peuvent être automatiques (modèle juge, similarité sémantique) ou humaines (notation). Garder un jeu de tests reproductible.", "metadata": {"source": "agents_ia", "type": "evaluation", "title": "Métriques d'évaluation"}},
    {"content": "Tests de régression pour RAG : constituer un jeu de paires (question, réponse attendue ou critères). Après chaque modification (prompt, chunking, retriever), relancer les requêtes et comparer les réponses ou les sources. Détecter les régressions (sources moins pertinentes, hallucinations).", "metadata": {"source": "agents_ia", "type": "evaluation", "title": "Tests de régression RAG"}},
    {"content": "Fidélité aux sources (groundedness) : vérifier que chaque affirmation de la réponse est supportée par un passage du contexte RAG. On peut utiliser un second LLM pour vérifier phrase par phrase ou calculer un score d'entailment. Une réponse fidèle évite les hallucinations sur des faits.", "metadata": {"source": "agents_ia", "type": "evaluation", "title": "Fidélité aux sources"}},
    {"content": "Chunking pour RAG : découper les documents en segments de taille adaptée (ex. 500–800 caractères) avec un overlap (ex. 100 caractères) pour éviter de couper au milieu d'une phrase. Utiliser RecursiveCharacterTextSplitter pour respecter les paragraphes. Les métadonnées (source, titre) doivent être propagées à chaque chunk.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Chunking RAG"}},
    {"content": "Seuil de similarité en RAG : après la recherche vectorielle, filtrer les chunks dont la similarité (ou 1 - distance) est en dessous d'un seuil (ex. 0.65) pour ne pas injecter du bruit dans le prompt. Cela améliore la qualité des réponses et réduit les citations hors-sujet.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Seuil de similarité"}},
    {"content": "System prompt pour un expert RAG : indiquer clairement le rôle (ex. expert en agents IA), exiger de ne répondre qu'à partir du contexte fourni, demander de citer les sources (titre ou extrait), et dire explicitement si le contexte ne contient pas l'information. Cela limite les hallucinations et aide l'utilisateur à challenger.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "System prompt expert RAG"}},
    {"content": "Agents multi-étapes : pour les tâches complexes, décomposer en sous-objectifs et laisser l'agent enchaîner les étapes (plan-and-execute ou ReAct). Garder un état explicite (contexte, résultats intermédiaires) et un budget max d'étapes pour éviter les boucles et les coûts excessifs.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Agents multi-étapes"}},
    {"content": "Mémoire et contexte : un agent peut conserver un historique des échanges (messages) ou un résumé. Pour les fenêtres limitées, résumer les anciens tours ou ne garder que les N derniers messages. La mémoire peut être stockée en base ou en session pour une persistance entre requêtes.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Mémoire et contexte"}},
    {"content": "Validation des entrées utilisateur : avant d'appeler des outils ou d'injecter du contenu dans un prompt, valider et sanitizer les entrées (longueur, caractères, schéma). Éviter l'injection de prompt en ne faisant pas confiance aveuglément au texte utilisateur dans les templates.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Validation des entrées"}},
    {"content": "Logging et observabilité : tracer chaque appel LLM (modèle, tokens, coût), chaque appel d'outil (nom, arguments, résultat ou erreur) et la sortie finale. Cela permet de déboguer, d'optimiser les coûts et d'auditer le comportement de l'agent en production.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Logging et observabilité"}},
    {"content": "RunnablePassthrough dans LangChain permet de faire passer des entrées à travers une chaîne tout en ajoutant des champs calculés. Par exemple assign(context=lambda x: retriever.invoke(x)) garde la question et ajoute le contexte récupéré. Utile pour construire des chaînes RAG où la question et le contexte sont tous deux nécessaires au prompt.", "metadata": {"source": "agents_ia", "type": "langchain", "title": "RunnablePassthrough"}},
    {"content": "Document LangChain : la classe Document a page_content (str) et metadata (dict). Les retriever et vector store travaillent avec des listes de Document. Les metadata (source, title, etc.) sont conservées tout au long du pipeline pour afficher les sources à l'utilisateur.", "metadata": {"source": "agents_ia", "type": "langchain", "title": "Document LangChain"}},
    {"content": "Comparaison similarité cosine et produit scalaire : pour des vecteurs normalisés, produit scalaire et cosine sont équivalents. La distance cosine = 1 - similarité. Plus la distance est faible, plus le document est pertinent. En Python : similarité = dot(a,b)/(norm(a)*norm(b)).", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Similarité cosine"}},
    {"content": "Filtrage par métadonnées en recherche vectorielle : pour limiter la recherche à une collection (ex. source = 'agents_ia'), filtrer les documents en base avec une clause sur metadata (ex. WHERE metadata @> '{\"source\": \"agents_ia\"}') avant de calculer la similarité. Réduit le bruit et permet plusieurs corpus dans la même table.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Filtrage par métadonnées"}},
    {"content": "Export de la base vectorisée pour audit : fournir un endpoint (ex. GET /rag/data) qui retourne tous les documents (id, content, metadata, optionnellement embedding) pour que l'on puisse vérifier le corpus et recalculer les similarités. Essentiel pour challenger les réponses du chatbot et reproduire les résultats.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Export base vectorisée"}},
    {"content": "Gestion des tours multiples en chat : stocker l'historique (user, assistant, tool results) et l'injecter dans le prompt à chaque tour. Tronquer ou résumer si la fenêtre est dépassée. Utiliser un message system stable et des messages human/assistant pour le dialogue. Les frameworks (LangChain, etc.) fournissent des abstractions pour les chaînes conversationnelles.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Tours multiples"}},
    {"content": "Choix du modèle LLM : les modèles plus grands (GPT-4, Claude) sont meilleurs pour le raisonnement et le tool use mais plus coûteux. Les modèles plus petits (GPT-4-mini, etc.) suffisent pour des tâches simples ou du RAG avec contexte court. Adapter le modèle au cas d'usage et au budget.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Choix du modèle"}},
    {"content": "Temperature et déterministe : pour des réponses reproductibles (ex. extraction de données, classification), utiliser temperature=0. Pour du dialogue ou de la créativité, une température plus élevée (0.3–0.7) donne de la variété. En RAG, une température basse (0.1–0.3) favorise la fidélité au contexte.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Temperature"}},
    {"content": "Structured output : pour obtenir des réponses structurées (JSON, liste de champs), utiliser les capacités du modèle (ex. response_format de l'API OpenAI) ou un prompt + PydanticOutputParser dans LangChain. Cela facilite l'intégration avec le reste du système (base de données, API).", "metadata": {"source": "agents_ia", "type": "langchain", "title": "Structured output"}},
    {"content": "Réessai et fallback : si un appel LLM échoue (rate limit, timeout), implémenter un retry avec backoff. En production, prévoir un fallback (message d'erreur utilisateur, modèle de repli) pour ne pas laisser l'utilisateur sans réponse. Logger les échecs pour analyse.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Réessai et fallback"}},
    {"content": "Coût par requête RAG : inclut l'embedding de la question (1 appel embedding), la recherche vectorielle (sans coût API si faite en local), et l'appel LLM (prompt = contexte + question, completion = réponse). Optimiser en réduisant la taille des chunks ou le nombre k de chunks récupérés.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Coût par requête RAG"}},
    {"content": "Évaluation de la pertinence du retrieval : pour une question, vérifier que les chunks retournés par le retriever contiennent bien l'information attendue. Métriques possibles : recall@k (la bonne source est-elle dans les k premiers), MRR, ou notation humaine. Améliorer le chunking ou l'embedding si le recall est faible.", "metadata": {"source": "agents_ia", "type": "evaluation", "title": "Pertinence du retrieval"}},
    {"content": "Pipeline d'ingestion RAG : (1) charger les documents sources (fichiers, JSON), (2) découper en chunks (RecursiveCharacterTextSplitter), (3) embedder les chunks (OpenAIEmbeddings), (4) stocker en base (content, metadata, embedding). Répéter l'ingestion quand le corpus change.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Pipeline ingestion"}},
    {"content": "Pipeline de requête RAG : (1) embedder la question, (2) recherche vectorielle (similarity_search) avec k et éventuel filtre metadata, (3) filtrer par score si besoin (score_threshold), (4) formater les chunks en contexte, (5) construire le prompt (system + contexte + question), (6) appeler le LLM, (7) retourner réponse et sources avec score.", "metadata": {"source": "agents_ia", "type": "pattern", "title": "Pipeline requête RAG"}},
    {"content": "BaseRetriever LangChain : classe de base pour les retriever. Implémenter _get_relevant_documents (sync) ou _aget_relevant_documents (async). Le retriever reçoit une requête et retourne une liste de Document. S'intègre dans une chaîne avec | pour alimenter un prompt en contexte.", "metadata": {"source": "agents_ia", "type": "langchain", "title": "BaseRetriever"}},
    {"content": "ChatOpenAI LangChain : wrapper du client OpenAI pour les modèles chat (gpt-4, gpt-3.5-turbo, etc.). Paramètres courants : model, api_key, temperature, max_tokens. Méthodes invoke/ainvoke prennent une liste de messages (ou un dict avec variables). Utiliser avec un ChatPromptTemplate pour formater les messages.", "metadata": {"source": "agents_ia", "type": "langchain", "title": "ChatOpenAI"}},
    {"content": "Déploiement d'agents : en production, exposer l'agent via une API (REST ou WebSocket pour le streaming). Gérer l'authentification, les quotas et le rate limiting. Monitorer la latence, les erreurs et les coûts. Prévoir des tests de charge si le trafic est important.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Déploiement"}},
    {"content": "Streaming des réponses : pour une meilleure UX, streamer le contenu du LLM token par token (stream=True dans l'API OpenAI). Côté chaîne LangChain, utiliser .stream() ou .astream(). Adapter le format de réponse (SSE, WebSocket) pour le frontend.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Streaming"}},
    {"content": "Sécurité des outils : chaque outil peut être dangereux (exécution de code, accès réseau). Valider les paramètres, limiter les outils exposés à l'agent, et appliquer le principe du moindre privilège. Ne pas permettre d'outils qui modifient le système ou les données sensibles sans contrôle.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Sécurité des outils"}},
    {"content": "Réutilisation du cache : pour des requêtes identiques ou très similaires, mettre en cache la réponse ou au moins les résultats du retriever. Réduit la latence et le coût. Invalider le cache quand le corpus ou le prompt change.", "metadata": {"source": "agents_ia", "type": "best_practice", "title": "Cache"}},
    {"content": "Documentation pour le prof : fournir un document (ex. RAG.md) décrivant l'architecture (ingestion, requête), la formule de similarité, les rôles de chaque fonction (chunking, store, retriever, chain) et la procédure pour obtenir la data vectorisée et challenger les réponses. Facilite l'évaluation et les questions sur le code.", "metadata": {"source": "agents_ia", "type": "evaluation", "title": "Documentation pour évaluation"}}
  ]
}
